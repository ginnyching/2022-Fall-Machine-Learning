{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66k3Z7o2w3Ou"
      },
      "source": [
        "## HW3: Decision Tree, AdaBoost and Random Forest\n",
        "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
        "\n",
        "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oawEW9U6w3Ox"
      },
      "source": [
        "## Question 1\n",
        "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfqN1D4_xoPL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flFpfDJHzud6"
      },
      "source": [
        "$Gini=1-\\Sigma_{j}p^{2}_{j}$\n",
        "\n",
        "$Entropy=-1*\\Sigma_{j}  p_{j}log_{2}p_{j}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1Aysswhw3Oy"
      },
      "outputs": [],
      "source": [
        "# Copy and paste your implementations right here to check your result\n",
        "# (Of course you can add your classes not written here)\n",
        "def gini(sequence):\n",
        "  labels, count = np.unique(sequence, return_counts=True)\n",
        "  #label=[1,2], count=[7,4]\n",
        "  probability = count/len(sequence)\n",
        "  return 1-np.sum(probability**2)\n",
        "\n",
        "def entropy(sequence):\n",
        "  labels, count = np.unique(sequence, return_counts=True)\n",
        "  probability = count/len(sequence)\n",
        "  return -1*np.sum(probability*np.log2(probability))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhhFBZmlw3O0"
      },
      "outputs": [],
      "source": [
        "# 1 = class 1,\n",
        "# 2 = class 2\n",
        "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cTd11NLw3O0",
        "outputId": "be00fbfe-5974-47f9-95ea-227cdc5faff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gini of data is  0.4628099173553719\n"
          ]
        }
      ],
      "source": [
        "print(\"Gini of data is \", gini(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q0IqXQew3O1",
        "outputId": "90e80463-40d8-4bbe-ea6a-a2c813a7d929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entropy of data is  0.9456603046006401\n"
          ]
        }
      ],
      "source": [
        "print(\"Entropy of data is \", entropy(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-UXiShsw3O2"
      },
      "source": [
        "## Load data\n",
        "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "dFr_kQwgw3O2",
        "outputId": "05861bed-6f44-4075-dc18-5b40b25e84a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1200, 21)\n",
            "(300, 21)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4869f886-d9fe-4eb9-943c-9a1d74a975d1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>blue</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>fc</th>\n",
              "      <th>four_g</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>px_height</th>\n",
              "      <th>px_width</th>\n",
              "      <th>ram</th>\n",
              "      <th>sc_h</th>\n",
              "      <th>sc_w</th>\n",
              "      <th>talk_time</th>\n",
              "      <th>three_g</th>\n",
              "      <th>touch_screen</th>\n",
              "      <th>wifi</th>\n",
              "      <th>price_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1583</td>\n",
              "      <td>1</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0.7</td>\n",
              "      <td>148</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>942</td>\n",
              "      <td>1651</td>\n",
              "      <td>1704</td>\n",
              "      <td>17</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>745</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>0.8</td>\n",
              "      <td>102</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>89</td>\n",
              "      <td>1538</td>\n",
              "      <td>2459</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>832</td>\n",
              "      <td>0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>0.7</td>\n",
              "      <td>103</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>125</td>\n",
              "      <td>1504</td>\n",
              "      <td>1799</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1175</td>\n",
              "      <td>1</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0.3</td>\n",
              "      <td>164</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>873</td>\n",
              "      <td>1394</td>\n",
              "      <td>1944</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>695</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0.6</td>\n",
              "      <td>196</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>1649</td>\n",
              "      <td>1829</td>\n",
              "      <td>2855</td>\n",
              "      <td>16</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4869f886-d9fe-4eb9-943c-9a1d74a975d1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4869f886-d9fe-4eb9-943c-9a1d74a975d1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4869f886-d9fe-4eb9-943c-9a1d74a975d1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
              "0           1583     1          2.1         1  11       0          14    0.7   \n",
              "1            745     1          0.6         1   5       0          35    0.8   \n",
              "2            832     0          0.7         1   2       1          39    0.7   \n",
              "3           1175     1          1.3         0   2       0          19    0.3   \n",
              "4            695     0          0.5         0  18       1          12    0.6   \n",
              "\n",
              "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
              "0        148        7  ...        942      1651  1704    17    13          2   \n",
              "1        102        8  ...         89      1538  2459    14     1         16   \n",
              "2        103        4  ...        125      1504  1799     5     2         11   \n",
              "3        164        7  ...        873      1394  1944     9     4          9   \n",
              "4        196        2  ...       1649      1829  2855    16    13          7   \n",
              "\n",
              "   three_g  touch_screen  wifi  price_range  \n",
              "0        1             0     1            1  \n",
              "1        1             1     0            0  \n",
              "2        1             0     1            0  \n",
              "3        1             1     0            0  \n",
              "4        1             1     1            1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('train.csv')\n",
        "val_df = pd.read_csv('val.csv')\n",
        "print(train_df.shape)\n",
        "print(val_df.shape)\n",
        "\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhFBCm0VFfV_",
        "outputId": "5d34bbec-b13e-4da2-a301-1642ec217c22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "battery_power: {501, 503, 504, 506, 507, 508, 509, 510, 511, 514, 518, 519, 523, 525, 527, 528, 530, 532, 534, 535, 536, 537, 538, 541, 543, 545, 547, 550, 551, 553, 555, 557, 560, 561, 563, 564, 565, 568, 569, 571, 574, 576, 578, 579, 580, 581, 582, 583, 584, 586, 589, 590, 592, 593, 594, 595, 596, 598, 600, 601, 602, 603, 608, 609, 612, 614, 615, 616, 617, 618, 622, 623, 625, 626, 627, 628, 630, 635, 637, 640, 641, 642, 643, 645, 648, 649, 652, 659, 663, 664, 667, 668, 671, 672, 673, 675, 680, 681, 682, 683, 685, 686, 687, 688, 689, 691, 694, 695, 696, 697, 702, 704, 706, 707, 708, 709, 710, 712, 713, 714, 717, 718, 719, 720, 721, 722, 723, 724, 726, 728, 729, 730, 731, 732, 733, 739, 740, 742, 744, 745, 748, 752, 753, 754, 755, 757, 763, 764, 765, 767, 768, 769, 770, 771, 772, 774, 775, 786, 787, 790, 793, 794, 797, 798, 799, 802, 803, 805, 807, 808, 809, 811, 816, 818, 822, 823, 825, 826, 827, 829, 831, 832, 833, 837, 838, 839, 840, 841, 843, 848, 851, 852, 855, 856, 860, 862, 864, 865, 866, 868, 869, 871, 874, 875, 878, 879, 880, 881, 885, 890, 891, 892, 893, 894, 895, 897, 899, 902, 903, 904, 907, 908, 909, 911, 912, 914, 915, 916, 917, 918, 920, 921, 922, 924, 925, 926, 928, 932, 935, 936, 940, 942, 946, 947, 948, 950, 954, 955, 956, 957, 959, 961, 962, 964, 965, 966, 969, 972, 973, 977, 979, 981, 984, 986, 987, 989, 990, 993, 994, 995, 999, 1000, 1002, 1003, 1004, 1006, 1008, 1009, 1010, 1012, 1018, 1020, 1021, 1028, 1030, 1031, 1034, 1035, 1039, 1042, 1043, 1045, 1048, 1049, 1052, 1053, 1054, 1057, 1058, 1059, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1072, 1074, 1076, 1077, 1081, 1082, 1083, 1084, 1086, 1087, 1090, 1092, 1093, 1095, 1097, 1099, 1100, 1101, 1103, 1104, 1106, 1108, 1109, 1110, 1112, 1113, 1117, 1119, 1122, 1125, 1126, 1127, 1128, 1129, 1130, 1133, 1135, 1136, 1138, 1142, 1143, 1146, 1147, 1149, 1150, 1152, 1154, 1156, 1157, 1158, 1159, 1160, 1161, 1163, 1165, 1166, 1168, 1171, 1172, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1188, 1189, 1190, 1193, 1195, 1197, 1199, 1201, 1202, 1205, 1207, 1210, 1216, 1218, 1219, 1220, 1221, 1224, 1225, 1227, 1229, 1230, 1231, 1232, 1234, 1236, 1237, 1239, 1242, 1245, 1248, 1250, 1251, 1253, 1254, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1269, 1271, 1272, 1273, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1283, 1284, 1285, 1288, 1289, 1296, 1299, 1303, 1307, 1308, 1309, 1310, 1312, 1313, 1314, 1318, 1320, 1322, 1327, 1328, 1329, 1330, 1331, 1333, 1339, 1341, 1342, 1344, 1345, 1347, 1348, 1349, 1350, 1352, 1355, 1356, 1358, 1361, 1362, 1366, 1367, 1369, 1371, 1373, 1375, 1379, 1387, 1394, 1395, 1396, 1398, 1402, 1403, 1404, 1405, 1407, 1408, 1410, 1412, 1413, 1414, 1417, 1421, 1423, 1424, 1425, 1426, 1428, 1429, 1430, 1431, 1432, 1436, 1438, 1442, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1454, 1456, 1457, 1462, 1464, 1467, 1469, 1470, 1472, 1476, 1479, 1481, 1482, 1483, 1484, 1485, 1488, 1489, 1490, 1492, 1494, 1496, 1497, 1498, 1502, 1504, 1510, 1512, 1514, 1517, 1519, 1520, 1523, 1524, 1526, 1527, 1528, 1530, 1533, 1537, 1539, 1540, 1542, 1544, 1545, 1546, 1547, 1549, 1550, 1551, 1552, 1554, 1558, 1559, 1561, 1562, 1563, 1564, 1565, 1567, 1569, 1571, 1572, 1576, 1577, 1578, 1579, 1581, 1583, 1584, 1588, 1589, 1590, 1593, 1595, 1596, 1597, 1600, 1602, 1604, 1606, 1607, 1608, 1610, 1614, 1615, 1617, 1619, 1620, 1624, 1625, 1627, 1630, 1631, 1632, 1634, 1635, 1640, 1641, 1642, 1644, 1645, 1646, 1647, 1648, 1653, 1654, 1655, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1667, 1670, 1671, 1672, 1673, 1677, 1678, 1685, 1686, 1689, 1692, 1695, 1696, 1697, 1698, 1699, 1701, 1702, 1703, 1707, 1708, 1709, 1712, 1715, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1727, 1729, 1730, 1731, 1732, 1735, 1741, 1742, 1748, 1753, 1757, 1759, 1762, 1763, 1765, 1766, 1770, 1772, 1773, 1776, 1777, 1778, 1779, 1780, 1783, 1784, 1786, 1788, 1790, 1792, 1793, 1794, 1799, 1800, 1802, 1804, 1805, 1807, 1808, 1809, 1811, 1812, 1815, 1816, 1820, 1821, 1824, 1827, 1829, 1830, 1831, 1834, 1835, 1836, 1837, 1840, 1841, 1845, 1846, 1848, 1849, 1851, 1853, 1854, 1855, 1856, 1859, 1860, 1861, 1862, 1864, 1866, 1867, 1868, 1869, 1871, 1872, 1874, 1875, 1876, 1878, 1880, 1881, 1883, 1884, 1885, 1886, 1887, 1889, 1891, 1893, 1895, 1897, 1898, 1899, 1900, 1901, 1902, 1905, 1906, 1908, 1910, 1911, 1918, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1933, 1934, 1936, 1937, 1938, 1940, 1944, 1945, 1946, 1948, 1949, 1954, 1957, 1958, 1959, 1960, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1973, 1974, 1976, 1977, 1979, 1982, 1983, 1986, 1988, 1989, 1991, 1992, 1993, 1994, 1995, 1996, 1997}\n",
            "blue: {0, 1}\n",
            "clock_speed: {0.6, 0.7, 2.1, 1.3, 0.5, 1.8, 0.9, 1.2, 1.0, 2.8, 2.9, 2.7, 2.0, 1.5, 2.5, 3.0, 1.6, 1.1, 2.6, 1.7, 2.3, 2.2, 0.8, 1.9, 1.4, 2.4}\n",
            "dual_sim: {0, 1}\n",
            "fc: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}\n",
            "four_g: {0, 1}\n",
            "int_memory: {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64}\n",
            "m_dep: {0.7, 0.8, 0.6, 0.3, 0.4, 0.5, 0.1, 0.9, 0.2, 1.0}\n",
            "mobile_wt: {80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200}\n",
            "n_cores: {1, 2, 3, 4, 5, 6, 7, 8}\n",
            "px_height: {0, 2, 3, 4, 6, 7, 8, 9, 11, 15, 17, 18, 19, 20, 23, 26, 29, 30, 32, 35, 36, 39, 40, 42, 43, 44, 46, 48, 50, 51, 52, 53, 56, 59, 62, 64, 65, 66, 67, 68, 71, 73, 75, 77, 78, 79, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 91, 94, 95, 96, 97, 98, 103, 104, 105, 108, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 125, 126, 127, 129, 130, 134, 135, 137, 138, 140, 142, 144, 146, 148, 149, 150, 151, 153, 154, 156, 157, 159, 160, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 176, 177, 178, 179, 180, 181, 182, 183, 184, 186, 187, 188, 190, 191, 192, 193, 194, 195, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 217, 221, 223, 224, 225, 227, 228, 229, 231, 233, 234, 235, 238, 239, 240, 241, 242, 245, 246, 247, 248, 250, 251, 253, 256, 257, 258, 262, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 282, 283, 284, 286, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 302, 304, 306, 308, 311, 313, 314, 315, 317, 321, 322, 323, 325, 327, 329, 330, 331, 332, 333, 335, 336, 337, 342, 343, 344, 346, 347, 349, 351, 354, 356, 358, 361, 363, 364, 366, 369, 370, 371, 372, 375, 376, 380, 381, 382, 384, 385, 386, 387, 388, 389, 390, 391, 393, 394, 395, 396, 397, 398, 400, 401, 403, 404, 406, 407, 408, 409, 410, 412, 416, 417, 418, 419, 423, 424, 426, 429, 430, 431, 433, 434, 436, 437, 439, 441, 442, 443, 444, 447, 448, 449, 450, 451, 453, 454, 455, 457, 459, 461, 463, 466, 467, 468, 469, 470, 471, 473, 477, 478, 480, 482, 484, 485, 486, 490, 491, 492, 493, 494, 495, 496, 497, 499, 501, 503, 504, 507, 509, 510, 511, 512, 517, 518, 519, 520, 521, 522, 523, 526, 527, 531, 532, 533, 534, 538, 540, 541, 542, 543, 545, 546, 547, 550, 552, 553, 554, 557, 558, 560, 561, 562, 563, 564, 569, 570, 571, 573, 574, 575, 576, 577, 578, 579, 581, 583, 584, 586, 588, 589, 590, 593, 594, 597, 599, 600, 602, 603, 605, 606, 607, 609, 610, 611, 613, 614, 616, 617, 619, 622, 623, 624, 626, 629, 630, 632, 634, 635, 637, 638, 639, 641, 647, 648, 649, 650, 651, 653, 654, 655, 657, 659, 661, 662, 663, 664, 665, 666, 667, 670, 671, 672, 674, 680, 681, 683, 685, 688, 689, 690, 691, 692, 693, 699, 701, 703, 704, 706, 707, 710, 712, 713, 714, 715, 717, 718, 719, 721, 724, 725, 727, 729, 730, 737, 740, 741, 743, 744, 747, 751, 753, 754, 755, 757, 758, 759, 760, 761, 762, 764, 765, 766, 767, 768, 769, 774, 776, 777, 778, 781, 782, 785, 786, 788, 789, 793, 798, 800, 803, 804, 805, 806, 808, 809, 810, 811, 812, 818, 819, 820, 821, 824, 825, 829, 831, 832, 833, 835, 836, 837, 838, 839, 840, 846, 849, 852, 853, 854, 858, 859, 861, 862, 866, 868, 871, 873, 874, 875, 879, 881, 882, 885, 886, 887, 888, 889, 890, 897, 898, 902, 903, 905, 910, 911, 912, 913, 915, 916, 919, 920, 924, 925, 927, 928, 930, 931, 934, 935, 937, 938, 942, 944, 945, 946, 948, 950, 951, 954, 956, 957, 961, 964, 970, 975, 977, 979, 980, 982, 983, 984, 985, 986, 987, 988, 990, 991, 992, 993, 994, 1003, 1004, 1010, 1011, 1012, 1013, 1015, 1016, 1025, 1027, 1028, 1041, 1042, 1043, 1045, 1047, 1048, 1052, 1053, 1054, 1055, 1057, 1058, 1064, 1074, 1076, 1078, 1079, 1081, 1083, 1085, 1088, 1094, 1096, 1097, 1099, 1100, 1103, 1105, 1108, 1109, 1111, 1112, 1116, 1117, 1119, 1122, 1123, 1125, 1127, 1128, 1131, 1132, 1135, 1137, 1138, 1145, 1150, 1151, 1153, 1157, 1158, 1161, 1163, 1168, 1170, 1171, 1172, 1175, 1176, 1177, 1178, 1179, 1180, 1184, 1185, 1187, 1188, 1191, 1194, 1201, 1203, 1208, 1209, 1210, 1211, 1213, 1214, 1215, 1217, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1230, 1233, 1236, 1240, 1242, 1243, 1244, 1248, 1250, 1252, 1259, 1261, 1262, 1263, 1272, 1273, 1274, 1277, 1279, 1281, 1284, 1285, 1287, 1290, 1294, 1298, 1299, 1301, 1304, 1307, 1311, 1314, 1315, 1323, 1330, 1331, 1332, 1333, 1335, 1339, 1345, 1347, 1352, 1360, 1362, 1364, 1365, 1371, 1385, 1386, 1391, 1397, 1399, 1404, 1406, 1417, 1419, 1420, 1428, 1438, 1442, 1445, 1446, 1451, 1453, 1466, 1471, 1480, 1482, 1485, 1495, 1502, 1518, 1524, 1528, 1530, 1537, 1552, 1563, 1565, 1571, 1573, 1578, 1580, 1596, 1597, 1605, 1613, 1617, 1619, 1626, 1638, 1641, 1649, 1652, 1658, 1661, 1684, 1686, 1693, 1694, 1698, 1703, 1706, 1709, 1713, 1728, 1738, 1749, 1750, 1770, 1790, 1791, 1795, 1801, 1802, 1826, 1836, 1852, 1874, 1878, 1895, 1901, 1914, 1920, 1960}\n",
            "px_width: {501, 503, 506, 507, 508, 509, 510, 511, 512, 513, 515, 516, 517, 519, 520, 527, 529, 530, 533, 537, 539, 540, 541, 542, 544, 545, 547, 548, 550, 551, 556, 557, 558, 559, 562, 563, 564, 565, 568, 571, 572, 574, 580, 581, 583, 584, 588, 589, 590, 591, 596, 598, 603, 605, 613, 614, 615, 616, 618, 620, 621, 622, 623, 627, 629, 630, 631, 636, 637, 638, 639, 640, 642, 643, 644, 645, 648, 650, 651, 654, 655, 656, 658, 660, 661, 662, 663, 665, 669, 670, 671, 672, 673, 674, 675, 676, 678, 679, 681, 682, 683, 684, 687, 688, 692, 695, 696, 699, 700, 703, 704, 705, 707, 708, 709, 710, 711, 712, 713, 714, 718, 721, 722, 723, 728, 730, 732, 734, 735, 736, 737, 738, 739, 740, 741, 745, 748, 750, 751, 754, 755, 756, 758, 759, 760, 763, 764, 765, 769, 770, 771, 772, 773, 778, 783, 786, 788, 789, 790, 791, 792, 793, 794, 797, 800, 802, 807, 809, 810, 811, 812, 814, 815, 816, 817, 819, 820, 821, 822, 823, 826, 828, 829, 831, 832, 836, 839, 840, 844, 846, 848, 849, 850, 851, 853, 854, 855, 857, 858, 859, 860, 862, 863, 864, 867, 869, 870, 872, 873, 874, 875, 876, 877, 881, 882, 884, 886, 887, 889, 891, 892, 893, 895, 896, 898, 902, 904, 906, 908, 909, 910, 912, 915, 917, 922, 925, 928, 930, 935, 938, 939, 941, 946, 947, 951, 952, 954, 956, 957, 962, 963, 966, 967, 969, 970, 973, 974, 975, 977, 978, 980, 981, 982, 983, 984, 986, 989, 990, 991, 992, 994, 998, 1000, 1001, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1016, 1018, 1019, 1020, 1021, 1022, 1023, 1025, 1026, 1027, 1029, 1030, 1032, 1033, 1035, 1036, 1037, 1038, 1039, 1040, 1042, 1043, 1046, 1048, 1049, 1050, 1051, 1052, 1053, 1055, 1057, 1058, 1062, 1064, 1066, 1069, 1073, 1075, 1076, 1077, 1078, 1079, 1081, 1082, 1083, 1084, 1087, 1088, 1089, 1090, 1096, 1099, 1101, 1103, 1104, 1105, 1106, 1108, 1109, 1111, 1113, 1116, 1118, 1119, 1123, 1126, 1127, 1129, 1131, 1132, 1133, 1134, 1135, 1136, 1139, 1143, 1144, 1147, 1149, 1151, 1155, 1156, 1157, 1158, 1159, 1161, 1162, 1163, 1167, 1168, 1169, 1170, 1171, 1173, 1175, 1176, 1178, 1179, 1182, 1184, 1185, 1190, 1191, 1195, 1196, 1197, 1199, 1203, 1206, 1208, 1211, 1212, 1215, 1219, 1220, 1222, 1224, 1225, 1226, 1227, 1228, 1230, 1232, 1233, 1234, 1235, 1238, 1241, 1242, 1243, 1244, 1245, 1247, 1248, 1252, 1255, 1259, 1261, 1262, 1263, 1264, 1267, 1269, 1272, 1274, 1278, 1280, 1284, 1287, 1288, 1290, 1291, 1292, 1293, 1294, 1299, 1300, 1301, 1302, 1304, 1306, 1307, 1308, 1311, 1312, 1313, 1315, 1316, 1317, 1318, 1326, 1327, 1330, 1331, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1343, 1344, 1345, 1347, 1348, 1350, 1351, 1352, 1353, 1354, 1356, 1358, 1359, 1360, 1364, 1366, 1370, 1371, 1374, 1376, 1378, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1391, 1393, 1394, 1396, 1398, 1400, 1402, 1403, 1404, 1405, 1407, 1409, 1411, 1413, 1415, 1416, 1418, 1420, 1421, 1422, 1423, 1424, 1426, 1427, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1440, 1441, 1442, 1445, 1448, 1452, 1453, 1455, 1456, 1458, 1462, 1463, 1464, 1465, 1468, 1469, 1471, 1472, 1473, 1476, 1477, 1478, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1491, 1492, 1495, 1496, 1497, 1499, 1500, 1503, 1504, 1506, 1508, 1509, 1510, 1511, 1514, 1517, 1518, 1520, 1523, 1524, 1529, 1531, 1532, 1533, 1534, 1536, 1538, 1539, 1540, 1543, 1544, 1545, 1550, 1551, 1552, 1553, 1554, 1558, 1564, 1565, 1566, 1569, 1570, 1571, 1575, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1586, 1591, 1595, 1596, 1602, 1604, 1605, 1606, 1608, 1609, 1611, 1613, 1615, 1616, 1617, 1618, 1621, 1622, 1623, 1629, 1630, 1632, 1633, 1635, 1636, 1638, 1639, 1641, 1643, 1645, 1646, 1647, 1648, 1649, 1651, 1652, 1654, 1655, 1657, 1660, 1661, 1662, 1663, 1664, 1666, 1667, 1670, 1671, 1673, 1675, 1676, 1677, 1681, 1682, 1686, 1687, 1688, 1692, 1697, 1698, 1702, 1703, 1705, 1709, 1710, 1716, 1717, 1718, 1719, 1721, 1722, 1723, 1724, 1726, 1727, 1728, 1731, 1732, 1733, 1737, 1738, 1739, 1740, 1742, 1743, 1744, 1746, 1748, 1753, 1754, 1759, 1760, 1761, 1763, 1764, 1767, 1768, 1769, 1771, 1775, 1777, 1780, 1781, 1782, 1785, 1787, 1790, 1791, 1793, 1794, 1796, 1798, 1802, 1803, 1804, 1805, 1806, 1807, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1819, 1827, 1829, 1831, 1832, 1836, 1838, 1839, 1841, 1846, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1857, 1858, 1862, 1864, 1865, 1866, 1870, 1873, 1876, 1877, 1878, 1880, 1882, 1883, 1884, 1886, 1889, 1890, 1891, 1892, 1893, 1895, 1896, 1897, 1898, 1899, 1903, 1904, 1905, 1910, 1911, 1913, 1914, 1916, 1917, 1919, 1920, 1922, 1923, 1924, 1925, 1928, 1929, 1930, 1931, 1933, 1935, 1937, 1942, 1944, 1947, 1948, 1950, 1951, 1954, 1956, 1957, 1958, 1962, 1963, 1964, 1965, 1966, 1967, 1970, 1972, 1973, 1975, 1976, 1977, 1981, 1983, 1985, 1987, 1988, 1989, 1994, 1995, 1996, 1997}\n",
            "ram: {2048, 2049, 2050, 2052, 2056, 2059, 2060, 2066, 2072, 2073, 2074, 2078, 2080, 2082, 2096, 2104, 2107, 2110, 2111, 2113, 2114, 2115, 2124, 2126, 2137, 2144, 2146, 2147, 2148, 2156, 2167, 2169, 2173, 2177, 2183, 2184, 2189, 2190, 2192, 2196, 2197, 2200, 2203, 2208, 2211, 2213, 2215, 2216, 2219, 2227, 2235, 2236, 2240, 2246, 2253, 2256, 2258, 2259, 2262, 2268, 2273, 2278, 2282, 2286, 2287, 2294, 2295, 2296, 2297, 2304, 256, 259, 2308, 2311, 2312, 2313, 263, 2315, 2316, 265, 267, 2319, 461, 273, 2322, 2323, 2321, 277, 2330, 284, 2334, 2335, 2336, 2337, 2338, 2339, 291, 292, 296, 298, 2346, 299, 301, 302, 2351, 305, 2355, 308, 309, 311, 312, 2361, 2360, 315, 2359, 2367, 2369, 323, 2372, 325, 2376, 331, 2381, 2382, 336, 2385, 337, 340, 2389, 2390, 2391, 343, 2392, 2394, 347, 348, 349, 2399, 2402, 2403, 354, 361, 363, 364, 2413, 368, 2419, 373, 374, 2430, 2437, 2438, 391, 2440, 2439, 2445, 398, 401, 402, 403, 404, 2454, 2456, 2459, 411, 2460, 2462, 412, 417, 418, 419, 2466, 422, 424, 2473, 2476, 429, 2478, 2479, 431, 435, 2484, 436, 437, 438, 2488, 440, 441, 2492, 2493, 446, 447, 448, 2495, 445, 452, 2501, 454, 2504, 2506, 2509, 462, 463, 2511, 465, 2513, 467, 468, 2517, 470, 2518, 2520, 473, 2523, 475, 478, 2528, 2532, 485, 488, 489, 490, 493, 2548, 503, 2552, 504, 505, 2554, 512, 513, 514, 515, 2562, 2563, 523, 2572, 524, 2574, 2575, 2571, 2577, 530, 532, 2583, 2587, 2589, 543, 2592, 545, 2593, 546, 2597, 2598, 2600, 552, 2603, 2606, 560, 2609, 2610, 2608, 2612, 2614, 2616, 568, 571, 574, 2622, 2623, 577, 2625, 2627, 582, 2631, 2633, 586, 587, 2636, 2637, 2638, 590, 593, 2641, 594, 2644, 595, 2648, 601, 2651, 604, 606, 609, 610, 2658, 616, 619, 2669, 2671, 624, 625, 2674, 2675, 2676, 629, 2678, 2677, 2680, 635, 2686, 639, 641, 643, 2693, 2698, 650, 2700, 651, 654, 656, 2705, 2710, 663, 2712, 2711, 665, 2715, 668, 667, 666, 670, 673, 2727, 2728, 681, 2734, 686, 2736, 688, 690, 2738, 2735, 2746, 700, 702, 2752, 2754, 707, 706, 711, 712, 716, 2764, 720, 2768, 722, 724, 725, 726, 2775, 2776, 728, 732, 733, 2782, 735, 2784, 2787, 739, 740, 2799, 752, 2800, 2802, 751, 757, 2806, 2811, 764, 2814, 769, 770, 2819, 774, 2822, 776, 2826, 780, 2829, 783, 2832, 785, 2836, 792, 2842, 794, 796, 2844, 2847, 799, 804, 2855, 808, 2858, 815, 816, 2865, 2863, 819, 820, 2870, 824, 832, 2885, 838, 841, 2890, 2893, 2895, 2896, 850, 2908, 861, 2910, 860, 864, 2915, 2916, 869, 872, 2921, 876, 878, 2927, 880, 2929, 881, 2933, 2934, 887, 885, 891, 2940, 2942, 2944, 896, 2945, 2948, 905, 2953, 2955, 909, 2958, 911, 2962, 916, 2965, 918, 2967, 920, 2968, 921, 2971, 928, 929, 2977, 2981, 934, 2982, 2984, 2986, 940, 941, 942, 2991, 2992, 950, 952, 953, 3006, 959, 961, 3012, 3015, 969, 3024, 978, 980, 3029, 3031, 984, 3034, 988, 990, 3038, 991, 999, 3054, 3056, 3057, 1012, 3063, 3064, 1017, 1018, 1019, 3068, 3066, 1022, 1026, 1027, 3077, 3078, 1032, 3083, 1037, 3086, 3087, 1044, 3094, 1046, 3097, 1050, 1052, 1053, 3104, 3105, 3112, 1066, 1067, 1068, 1069, 1070, 3117, 3121, 3122, 1077, 1078, 1082, 1083, 3132, 1086, 1087, 3139, 3141, 3142, 3143, 3144, 3153, 1105, 1107, 1112, 1113, 3161, 1115, 3167, 3169, 1122, 3173, 3176, 3178, 1133, 1138, 3187, 3190, 1145, 1146, 3197, 1149, 1150, 1152, 1155, 3204, 3206, 3208, 3209, 3210, 3212, 1165, 1164, 3215, 1167, 1172, 3220, 1175, 3226, 1179, 1181, 3230, 1183, 1184, 3235, 3238, 1193, 1196, 3248, 3249, 1204, 1205, 3254, 3255, 3256, 1209, 1210, 3259, 1212, 1208, 3262, 1214, 3267, 1220, 3269, 1223, 1228, 1229, 3278, 1234, 3282, 3284, 3286, 1241, 3291, 1243, 3293, 1246, 1244, 3297, 1251, 1252, 3300, 3302, 1257, 1258, 3314, 1267, 3317, 3321, 3322, 3323, 1276, 1277, 1275, 1274, 3328, 3336, 3338, 3340, 1295, 3348, 1301, 1302, 1303, 1305, 3355, 1308, 1309, 3359, 3362, 3366, 1321, 1322, 3372, 1324, 3376, 3377, 1329, 1333, 1334, 3383, 1336, 1337, 1338, 3387, 3388, 1341, 1342, 1339, 1344, 1345, 3392, 1348, 3397, 3396, 1352, 1354, 1356, 3406, 1360, 1362, 3411, 1366, 3416, 1370, 3421, 1375, 3424, 3426, 1379, 3429, 1382, 1384, 3433, 3437, 3438, 3441, 1394, 3442, 1396, 3447, 1400, 3449, 1402, 3451, 3448, 3454, 1406, 1409, 3458, 1410, 1412, 1414, 1419, 1424, 3473, 3472, 1427, 1430, 1432, 1433, 1434, 3483, 1436, 3484, 3486, 1440, 3488, 1441, 3490, 1444, 1445, 1446, 3494, 3497, 1449, 3499, 1454, 1457, 1462, 3511, 1464, 3510, 3518, 1470, 3520, 1471, 1475, 1480, 1482, 1484, 3534, 3535, 1489, 3537, 3538, 3541, 1503, 3554, 1507, 3557, 3563, 3565, 3566, 1519, 1518, 3568, 1524, 1529, 1534, 3585, 3586, 3587, 1539, 1542, 1543, 1545, 3595, 3598, 3600, 3601, 1552, 3606, 3607, 3608, 1561, 3610, 3614, 3615, 1568, 1567, 3619, 3622, 3623, 3624, 3625, 3629, 3635, 3637, 1591, 1593, 1595, 3646, 3647, 3652, 3653, 3654, 1607, 3655, 1609, 1610, 1614, 1620, 1622, 1624, 3672, 1628, 1629, 3676, 3681, 1633, 3684, 3685, 1641, 3692, 3693, 1647, 3696, 3695, 3699, 1652, 1653, 1655, 3703, 1656, 3705, 3704, 1663, 3713, 3714, 1667, 1666, 3717, 3716, 3720, 1675, 1686, 1687, 3736, 3739, 1692, 1693, 1695, 3744, 3745, 1699, 1704, 3752, 3755, 3760, 3761, 1713, 3763, 3764, 1717, 1716, 3762, 3771, 1724, 3773, 1726, 3774, 1725, 3777, 1731, 1733, 1734, 3784, 1737, 1742, 3791, 1744, 1747, 3796, 3798, 3799, 1751, 3801, 1754, 3803, 3800, 3809, 1762, 1767, 1769, 3817, 1774, 3822, 3825, 3826, 1780, 1781, 1783, 3833, 3834, 3835, 3836, 1788, 3838, 3839, 1796, 3844, 1798, 1799, 1797, 3846, 3845, 1808, 3860, 1813, 1816, 1817, 3868, 3869, 3872, 1824, 1832, 1834, 3883, 3885, 1837, 3887, 3892, 3894, 1846, 3897, 1851, 1853, 3902, 3904, 3905, 1861, 1862, 3911, 3912, 1866, 3914, 3916, 3917, 3918, 1869, 1870, 1871, 3922, 1875, 3915, 3925, 1878, 3927, 1882, 3930, 1885, 1886, 1887, 3933, 3940, 3941, 1896, 3945, 3946, 1900, 3952, 1905, 1904, 1906, 3955, 3957, 3958, 3961, 3962, 3963, 3964, 1913, 3966, 1919, 1921, 3971, 1927, 1930, 1936, 1938, 3990, 3991, 1944, 1945, 3993, 1947, 1948, 3996, 1955, 1958, 1965, 1968, 1970, 1971, 1973, 1974, 1998, 1999, 2002, 2013, 2020, 2022, 2029, 2032, 2033, 2039, 2042}\n",
            "sc_h: {5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}\n",
            "sc_w: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}\n",
            "talk_time: {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}\n",
            "three_g: {0, 1}\n",
            "touch_screen: {0, 1}\n",
            "wifi: {0, 1}\n",
            "price_range: {0, 1}\n"
          ]
        }
      ],
      "source": [
        "'''print(\"battery_power:\", set(train_df['battery_power']))\n",
        "print(\"blue:\", set(train_df['blue']))\n",
        "print(\"clock_speed:\", set(train_df['clock_speed']))\n",
        "print(\"dual_sim:\", set(train_df['dual_sim']))\n",
        "print(\"fc:\", set(train_df['fc']))\n",
        "print(\"four_g:\", set(train_df['four_g']))\n",
        "print(\"int_memory:\", set(train_df['int_memory']))\n",
        "print(\"m_dep:\", set(train_df['m_dep']))\n",
        "print(\"mobile_wt:\", set(train_df['mobile_wt']))\n",
        "print(\"n_cores:\", set(train_df['n_cores']))\n",
        "print(\"px_height:\", set(train_df['px_height']))\n",
        "print(\"px_width:\", set(train_df['px_width']))\n",
        "print(\"ram:\", set(train_df['ram']))\n",
        "print(\"sc_h:\", set(train_df['sc_h']))\n",
        "print(\"sc_w:\", set(train_df['sc_w']))\n",
        "print(\"talk_time:\", set(train_df['talk_time']))\n",
        "print(\"three_g:\", set(train_df['three_g']))\n",
        "print(\"touch_screen:\", set(train_df['touch_screen']))\n",
        "print(\"wifi:\", set(train_df['wifi']))\n",
        "print(\"price_range:\", set(train_df['price_range']))'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwWYvg0Gw3O3"
      },
      "source": [
        "## Question 2\n",
        "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
        "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
        "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BVc5MatNBcUA"
      },
      "outputs": [],
      "source": [
        "criterions = {'gini': gini,'entropy': entropy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "j19ERjo8w3O4"
      },
      "outputs": [],
      "source": [
        "class DecisionTree():\n",
        "    def __init__(self, criterion='gini', max_depth=None):\n",
        "        self.criterion_name = criterion\n",
        "        self.criterion = criterions[criterion]\n",
        "        self.max_depth = max_depth\n",
        "        self.root = None\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "        self.feature_names = None\n",
        "        self.importance_dict = None\n",
        "\n",
        "    def fit(self, x_data, y_data):\n",
        "        self.feature_names = list(x_data.columns)\n",
        "        self.importance_dict = dict(zip(self.feature_names, [0]*len(self.feature_names)))\n",
        "        self.x = x_data\n",
        "        self.y = y_data\n",
        "        x_copy = x_data.copy()\n",
        "        y_copy = y_data.copy()\n",
        "        x_copy['target'] = y_copy\n",
        "        self.root = TreeNode(x_copy, y_copy, self, 0)\n",
        "        self.root.BuildTree()\n",
        "\n",
        "        return None\n",
        "\n",
        "    def predict(self, x_data):\n",
        "        y_pred = []\n",
        "        for index in range(len(x_data)):\n",
        "            #set root to node\n",
        "            Node = self.root\n",
        "            #use iloc to get row index of x_data \n",
        "            x = x_data.iloc[index, :]\n",
        "            #while have child, left<=right\n",
        "            while ((Node.leftNode is not None) or (Node.rightNode is not None)):\n",
        "                if(x[Node.criterion_attr] <= Node.criterion_attr_value):\n",
        "                    Node = Node.leftNode\n",
        "                elif(x[Node.criterion_attr] > Node.criterion_attr_value):\n",
        "                    Node = Node.rightNode\n",
        "            if Node.predict is None:\n",
        "                Node = Node.parent\n",
        "            y_pred.append(Node.predict)\n",
        "        return y_pred\n",
        "    #count feature importance by calculating the time the feature has been used to split\n",
        "    def get_feature(self, Node):\n",
        "        #if feature being used before, importance+1\n",
        "        if(Node.criterion_attr in self.importance_dict):\n",
        "            self.importance_dict[Node.criterion_attr] += 1\n",
        "        #if feature is first used, add to importance\n",
        "        if((Node.leftNode is not None) and (Node.criterion_attr in self.importance_dict)):\n",
        "            self.get_feature(Node.leftNode)\n",
        "        if((Node.rightNode is not None) and (Node.criterion_attr in self.importance_dict)):\n",
        "            self.get_feature(Node.rightNode)\n",
        "    #plot function for Question3        \n",
        "    def plot_feature_importance(self):\n",
        "        self.get_feature(self.root)\n",
        "        plt.barh(list(self.importance_dict.keys()),list(self.importance_dict.values()))\n",
        "        plt.ylabel('feature names')\n",
        "        plt.xlabel('feature importance')\n",
        "        plt.yticks(list(self.importance_dict.keys()),list(self.importance_dict.keys()))\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hight=  8920"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jHZ0kBUpCP7Q"
      },
      "outputs": [],
      "source": [
        "class TreeNode():\n",
        "    def __init__(self, x, y, DT: DecisionTree, depth):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.DT = DT\n",
        "        self.depth = depth\n",
        "        self.min_criterion_value = 100000\n",
        "        self.criterion_attr_value = None\n",
        "        self.criterion_attr = None\n",
        "        self.predict = None\n",
        "        self.parent = None\n",
        "        self.leftNode = None\n",
        "        self.rightNode = None\n",
        "        self.criterion = self.DT.criterion(list(self.x[\"target\"]))\n",
        "\n",
        "    def SplitAttribute(self):\n",
        "\n",
        "        self.min_criterion_value = 100000\n",
        "        self.criterion_attr_value = None\n",
        "        self.criterion_attr = None\n",
        "\n",
        "        for attr in self.DT.feature_names:\n",
        "           \n",
        "            total_category_value = list(set(self.x[attr]))\n",
        "            total_category_value = sorted(total_category_value)\n",
        "           \n",
        "            for value in total_category_value:\n",
        "                #divide value to 2 parts, >=self.x[attr] and <self.x[attr]\n",
        "                A = self.x[self.x[attr] <= value]\n",
        "                B = self.x[self.x[attr] > value]\n",
        "                total = self.x[attr]\n",
        "                weightA = len(A)/len(total)\n",
        "                weightB = len(B)/len(total)\n",
        "                LabelA = A[\"target\"]\n",
        "                LabelB = B[\"target\"]\n",
        "                # calculate the criterion value\n",
        "                criterion_value = weightA *self.DT.criterion(LabelA) + weightB *self.DT.criterion(LabelB)\n",
        "                #if criterion value<min_criterion_value, change criterion_value to min_criterion_value\n",
        "                if(criterion_value < self.min_criterion_value):\n",
        "                    self.min_criterion_value = criterion_value\n",
        "                    self.criterion_attr_value = value\n",
        "                    self.criterion_attr = attr\n",
        "    def BuildTree(self):\n",
        "\n",
        "        if len(self.x) == 1:\n",
        "            final_predict = list(self.x[\"target\"])\n",
        "            self.predict = max(set(final_predict),key=final_predict.count)\n",
        "            #use return to terminate, it will take almost 30 times longer if return is not used \n",
        "            return\n",
        "        if (self.DT.max_depth is not None) and (self.depth == self.DT.max_depth):\n",
        "            final_predict = list(self.x[\"target\"])\n",
        "            self.predict = max(set(final_predict),key=final_predict.count)\n",
        "            #use return to terminate, it will take almost 30 times longer if return is not used \n",
        "            return\n",
        "        if self.criterion == 0:\n",
        "            final_predict = list(self.x[\"target\"])\n",
        "            self.predict = max(set(final_predict),key=final_predict.count)\n",
        "            #use return to terminate, it will take almost 30 times longer if return is not used \n",
        "            return\n",
        "        self.SplitAttribute()\n",
        "\n",
        "        A = self.x[self.x[self.criterion_attr] <= self.criterion_attr_value]\n",
        "        B = self.x[self.x[self.criterion_attr] > self.criterion_attr_value]\n",
        "\n",
        "        LabelA = A['target']\n",
        "        LabelB = B['target']\n",
        "\n",
        "\n",
        "        if(len(A) == 0):\n",
        "            self.leftNode = TreeNode(A, LabelA, self.DT, self.depth+1)\n",
        "            final_predict = list(self.x[\"target\"])\n",
        "            self.leftNode.redict = max(set(final_predict),key=final_predict.count)\n",
        "\n",
        "        elif(len(A) > 0):\n",
        "            self.leftNode = TreeNode(A, LabelA, self.DT, self.depth+1)\n",
        "            self.leftNode.parent = self\n",
        "\n",
        "            if(len(A) == len(self.x)) and ((A == self.x).all().all()):\n",
        "                final_predict = list(self.x[\"target\"])\n",
        "                self.predict = max(set(final_predict),key=final_predict.count)\n",
        "            else:\n",
        "                self.leftNode.BuildTree()\n",
        "\n",
        "        if(len(B) == 0):\n",
        "            self.rightNode = TreeNode(B, LabelB, self.DT, self.depth+1)\n",
        "            final_predict = list(self.x[\"target\"])\n",
        "            self.rightNode.predict = max(set(final_predict),key=final_predict.count)\n",
        "\n",
        "        elif(len(B) > 0):\n",
        "            self.rightNode = TreeNode(B, LabelB, self.DT, self.depth+1)\n",
        "            self.rightNode.parent = self\n",
        "\n",
        "            if(len(B) == len(self.x)) and ((B == self.x).all().all()):\n",
        "                final_predict = list(self.x[\"target\"])\n",
        "                self.predict = max(set(final_predict),key=final_predict.count)\n",
        "            else:\n",
        "                self.rightNode.BuildTree()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qXXJHjjw3O4"
      },
      "source": [
        "### Question 2.1\n",
        "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vlRpn4a0H7bG"
      },
      "outputs": [],
      "source": [
        "def accuracy(predict, answer):\n",
        "    predict = np.array(predict)\n",
        "    answer = np.array(answer)\n",
        "    return np.sum(predict == answer)/len(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "58mgQ8CcLDm3"
      },
      "outputs": [],
      "source": [
        "#wifi and price_range are the 2 feature I choose since they contain only 2 values {0,1}\n",
        "origin_wifi = []\n",
        "for i in list(train_df[\"wifi\"]):\n",
        "   if i not in origin_wifi:\n",
        "     origin_wifi.append(i)\n",
        "#use drop( ,axis=1) to delete column price_range\n",
        "x_train = train_df.drop(\"price_range\", axis=1)\n",
        "y_train = train_df[\"price_range\"]\n",
        "\n",
        "x_test = val_df.drop(\"price_range\", axis=1)\n",
        "y_test = val_df[\"price_range\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "5LrtrGV1w3O4"
      },
      "outputs": [],
      "source": [
        "#DT with max_depth=3\n",
        "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
        "#DT with max_depth=10\n",
        "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
        "#print(clf_depth3)\n",
        "#print(clf_depth10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "JS2OMCCLO5iW"
      },
      "outputs": [],
      "source": [
        "\n",
        "clf_depth3.fit(x_train, y_train)\n",
        "clf_depth10.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0UpEv1QO9jt",
        "outputId": "33a25b1f-f072-4b94-8c07-a54669346138"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DecisionTree(criterion='gini', max_depth=3), Accuracy Score: 0.9166666666666666\n",
            "DecisionTree(criterion='gini', max_depth=10), Accuracy Score: 0.9366666666666666\n"
          ]
        }
      ],
      "source": [
        "y_pred = clf_depth3.predict(x_test)\n",
        "print(\"DecisionTree(criterion='gini', max_depth=3), Accuracy Score:\",accuracy(y_pred, y_test))\n",
        "\n",
        "y_pred = clf_depth10.predict(x_test)\n",
        "print(\"DecisionTree(criterion='gini', max_depth=10), Accuracy Score:\",accuracy(y_pred, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAEk5gJhw3O5"
      },
      "source": [
        "### Question 2.2\n",
        "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "WBxOPYlkBLZS"
      },
      "outputs": [],
      "source": [
        "#DT with criterion='gini'\n",
        "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
        "#DT with criterion='entropy'\n",
        "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3mD92bAuBN4y"
      },
      "outputs": [],
      "source": [
        "clf_gini.fit(x_train, y_train)\n",
        "clf_entropy.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIwLwem-BPty",
        "outputId": "e7ee7def-6e3a-44b8-eb7b-42d3ee1037d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DecisionTree(criterion='gini', max_depth=3), Accuracy Score: 0.9166666666666666\n",
            "DecisionTree(criterion='entropy', max_depth=3), Accuracy Score: 0.93\n"
          ]
        }
      ],
      "source": [
        "y_pred = clf_gini.predict(x_test)\n",
        "print(\"DecisionTree(criterion='gini', max_depth=3), Accuracy Score:\",accuracy(y_pred, list(y_test)))\n",
        "\n",
        "y_pred = clf_entropy.predict(x_test)\n",
        "print(\"DecisionTree(criterion='entropy', max_depth=3), Accuracy Score:\",accuracy(y_pred, list(y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH00nlW8w3O5"
      },
      "source": [
        "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
        "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
        "- Hint: You can use the recursive method to build the nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSBXmco_w3O6"
      },
      "source": [
        "## Question 3\n",
        "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
        "\n",
        "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
        "\n",
        "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "PVhYaGuCvkuW",
        "outputId": "70015528-f934-45df-83da-351bd30b7f24"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "clf_depth10.plot_feature_importance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQA6KcTOw3O6"
      },
      "source": [
        "## Question 4\n",
        "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
        "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "C_Pi_baGw3O6"
      },
      "outputs": [],
      "source": [
        "class AdaBoost():\n",
        "    def __init__(self, n_estimators):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.total_error = 0\n",
        "        self.n_total_error = []\n",
        "        self.amount_value = None\n",
        "        self.Alldata = None\n",
        "        self.newAlldata = None\n",
        "        self.predict_dt = []\n",
        "        self.sample_weight = None\n",
        "\n",
        "    def fit(self, x_data, y_data):\n",
        "        self.Alldata = x_data.copy()\n",
        "        self.Alldata[\"target\"] = y_data.copy()\n",
        "        self.update_Alldata = self.Alldata.copy()\n",
        "\n",
        "        np.random.seed(12)\n",
        "        #for i in range(n_estimators)\n",
        "        for epochs in range(self.n_estimators):\n",
        "            #array[1/len(x_data),1/len(x_data),...,1/len(x_data)], len(array)=len(len(x_data))\n",
        "            self.sample_weight = np.full(len(x_data), 1/len(x_data))\n",
        "\n",
        "            if epochs == 0:\n",
        "                self.newAlldata = self.Alldata.copy()\n",
        "            else:\n",
        "                self.newAlldata = self.Alldata.copy()\n",
        "                self.newAlldata = self.newAlldata.sample(n=len(self.newAlldata), weights=list(self.sample_weight), replace=True, axis=0)\n",
        "\n",
        "            dt = DecisionTree(max_depth=1)\n",
        "            #drop->delete column \"target\"\n",
        "            dt.fit(self.newAlldata.drop(\"target\", axis=1),self.newAlldata[\"target\"])\n",
        "\n",
        "            self.target = np.array(self.newAlldata[\"target\"])\n",
        "\n",
        "            self.classification = np.array(dt.predict(self.newAlldata.drop(\"target\", axis=1)))\n",
        "            misclassification = self.sample_weight[self.target !=self.classification]\n",
        "\n",
        "            self.total_error = np.sum(misclassification)\n",
        "            self.amount_value = self.calculate_amount_value()\n",
        "            self.predict_dt.append([dt, self.amount_value])\n",
        "            self.update_sample_weight()\n",
        "\n",
        "    def predict(self, x_data):\n",
        "       \n",
        "        clf_predictions = np.array([np.array(dt.predict(x_data))for dt, weight in self.predict_dt])\n",
        "\n",
        "        predictions = []\n",
        "        for sample_predictions in clf_predictions.T:\n",
        "            #class relate to predictor_op\n",
        "            class_0 = 0\n",
        "            class_1 = 0\n",
        "            for estimators_idx, predictor_op in enumerate(sample_predictions):\n",
        "                if predictor_op == 0:\n",
        "                    class_0 += self.predict_dt[estimators_idx][1]\n",
        "                else:\n",
        "                    class_1 += self.predict_dt[estimators_idx][1]\n",
        "\n",
        "            if class_0 > class_1:\n",
        "                predictions.append(0)\n",
        "            else:\n",
        "                predictions.append(1)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def calculate_amount_value(self):\n",
        "\n",
        "        EPS = 1e-10\n",
        "\n",
        "        min_error = float('inf')\n",
        "        if(self.total_error > 0.5):\n",
        "            self.total_error = 1 - self.total_error\n",
        "        if(self.total_error < min_error):\n",
        "            min_error = self.total_error\n",
        "\n",
        "        return 0.5*(np.log((1-min_error)/(float(min_error)+EPS)))\n",
        "    def update_sample_weight(self):\n",
        "\n",
        "        used_signal = np.full(len(self.sample_weight), -1)\n",
        "        used_signal[self.target != self.classification] = 1\n",
        "\n",
        "        # calculate new weight\n",
        "        self.sample_weight = self.sample_weight *np.exp(used_signal*self.amount_value)\n",
        "\n",
        "        # normalize\n",
        "        self.sample_weight = self.sample_weight /np.sum(self.sample_weight)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OybLH6pw3O6"
      },
      "source": [
        "### Question 4.1\n",
        "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AUjs0oxw3O6",
        "outputId": "0a52939f-805c-4954-fc59-2b0bf942b79d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost(n_estimators=10), Accuracy Score: 0.8933333333333333\n",
            "AdaBoost(n_estimators=100), Accuracy Score: 0.8933333333333333\n"
          ]
        }
      ],
      "source": [
        "#ADABoost with n_estimators=10\n",
        "ad10 = AdaBoost(n_estimators=10)\n",
        "ad10.fit(x_train, y_train)\n",
        "y_pred = ad10.predict(x_test)\n",
        "print(\"AdaBoost(n_estimators=10), Accuracy Score:\", accuracy(y_pred, y_test))\n",
        "#ADABoost with n_estimators=100\n",
        "ad100 = AdaBoost(n_estimators=100)\n",
        "ad100.fit(x_train, y_train)\n",
        "y_pred = ad100.predict(x_test)\n",
        "print(\"AdaBoost(n_estimators=100), Accuracy Score:\", accuracy(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM7RY8yqw3O7"
      },
      "source": [
        "## Question 5\n",
        "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
        "\n",
        "1. **n_estimators**: The number of trees in the forest. \n",
        "2. **max_features**: The number of random select features to consider when looking for the best split\n",
        "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sRRxhuv8w3O7"
      },
      "outputs": [],
      "source": [
        "class RandomForest():\n",
        "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = int(max_features)\n",
        "        self.boostrap = boostrap\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.feature_names = None\n",
        "        self.classifiers = []\n",
        "    def fit(self, x_data, y_data):\n",
        "       np.random.seed()\n",
        "       #for i in range(n_estimators)\n",
        "       for estimator in range(self.n_estimators):\n",
        "            self.feature_names = np.array(list(x_data.columns))\n",
        "\n",
        "            np.random.shuffle(self.feature_names)\n",
        "            number_features = self.max_features\n",
        "            used_feature = self.feature_names[:number_features]\n",
        "\n",
        "            dt = DecisionTree(criterion=self.criterion,max_depth=self.max_depth)\n",
        "\n",
        "            if self.boostrap:\n",
        "                boostrap_data = x_data[used_feature]\n",
        "                boostrap_data = boostrap_data.copy()\n",
        "                #use np.random.choiceto get 1-D array of numpy having random samples \n",
        "                #numpy.random.choice(a, size=None, replace=True, p=None)\n",
        "                used_data_idx = np.random.choice(range(len(x_data)), len(x_data), replace=True)\n",
        "                \n",
        "                x_new, y_new = boostrap_data.iloc[used_data_idx], y_data.iloc[used_data_idx]\n",
        "\n",
        "                dt.fit(x_new, y_new)\n",
        "            else:\n",
        "                normal_data = x_data[used_feature]\n",
        "                normal_data = normal_data.copy()\n",
        "\n",
        "                x_new, y_new = normal_data, y_data\n",
        "\n",
        "                dt.fit(x_new, y_new)\n",
        "            self.classifiers.append([dt, used_feature])\n",
        "\n",
        "       return None\n",
        "\n",
        "    def predict(self, x_data):\n",
        "        predictions = [np.array(dt.predict(x_data[used_feature]))\n",
        "                       for dt, used_feature in self.classifiers]\n",
        "        predictions = np.array(predictions).T\n",
        "        for predict in predictions:\n",
        "          final_prediction = [Counter(predict).most_common(1)[0][0]]\n",
        "        return final_prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWmZ-K5vw3O7"
      },
      "source": [
        "### Question 5.1\n",
        "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sTmMLYIw3O7",
        "outputId": "ebf6858b-e360-4ca1-e8ee-9cad13aa9964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RandomForest(criterion='gini', n_estimators=10, max_features=np.sqrt(n_features), boostrap=True), Accuracy Score: 0.5266666666666666\n"
          ]
        }
      ],
      "source": [
        "\n",
        "clf_10tree = RandomForest(criterion=\"gini\",n_estimators=10, max_features=np.sqrt(x_train.shape[1]), boostrap=True)\n",
        "clf_10tree.fit(x_train, y_train)\n",
        "y_pred = clf_10tree.predict(x_test)\n",
        "print(\"RandomForest(criterion='gini', n_estimators=10, max_features=np.sqrt(n_features), boostrap=True), Accuracy Score:\",accuracy(y_pred, y_test))\n",
        "\n",
        "clf_100tree = RandomForest(criterion=\"gini\",n_estimators=100, max_features=np.sqrt(x_train.shape[1]))\n",
        "clf_100tree.fit(x_train, y_train)\n",
        "y_pred = clf_100tree.predict(x_test)\n",
        "print(\"RandomForest(criterion='gini', n_estimators=100, max_features=np.sqrt(n_features), boostrap=True), Accuracy Score:\",accuracy(y_pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmE7hy1w3O7"
      },
      "source": [
        "### Question 5.2\n",
        "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwuv8ZPDw3O7"
      },
      "outputs": [],
      "source": [
        "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(x_train.shape[1]))\n",
        "clf_all_features = RandomForest(n_estimators=10, max_features=x_train.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BePZHWNNw3O8"
      },
      "outputs": [],
      "source": [
        "clf_random_features.fit(x_train, y_train)\n",
        "y_pred = clf_random_features.predict(x_test)\n",
        "print(\"RandomForest(criterion='gini', n_estimators=10, max_features=np.sqrt(n_features), boostrap=True), Accuracy Score:\", accuracy(y_pred, y_test))\n",
        "clf_all_features.fit(x_train, y_train)\n",
        "y_pred = clf_all_features.predict(x_test)\n",
        "print(\"RandomForest(criterion='gini', n_estimators=10, max_features=n_features, boostrap=True), Accuracy Score:\",accuracy(y_pred, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWL50Xmow3O8"
      },
      "source": [
        "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBkFlFmDw3O8"
      },
      "source": [
        "### Question 6. Train and tune your model on a real-world dataset\n",
        "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
        "- Feature engineering\n",
        "- Hyperparameter tuning\n",
        "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "aUirqKpXw3O8"
      },
      "outputs": [],
      "source": [
        "def train_your_model(data):\n",
        "    max_accuracy = -1\n",
        "    model = None\n",
        "    ## Define your model and training \n",
        "    for depth in range(1, 10):\n",
        "      dt = DecisionTree(max_depth=depth)\n",
        "      dt.fit(x_train, y_train)\n",
        "      y_pred = dt.predict(x_test)\n",
        "      value = accuracy(y_pred, y_test)\n",
        "      if(value > max_accuracy):\n",
        "          #print(\"DecisionTree(criterion='gini', max_depth=\" +str(depth)+\"), Accuracy Score:\", value)\n",
        "          max_accuracy = value\n",
        "          model=dt\n",
        "\n",
        "    for ada_dt in range(1, 30):\n",
        "      dt = AdaBoost(n_estimators=ada_dt)\n",
        "      dt.fit(x_train, y_train)\n",
        "      y_pred = dt.predict(x_test)\n",
        "      value = accuracy(y_pred, y_test)\n",
        "      if(value > max_accuracy):\n",
        "          #print(\"AdaBoost(n_estimators=\"+str(ada_dt)+\"), Accuracy Score:\", value)\n",
        "          max_accuracy = value\n",
        "          model = dt\n",
        "    for rand_dt in range(1, 10):\n",
        "      for feature_num in range(2, x_test.shape[1]+1):\n",
        "          for boostrap in [True, False]:\n",
        "              dt = RandomForest(criterion=\"gini\",n_estimators=rand_dt, max_depth=4, max_features=feature_num, boostrap=boostrap)\n",
        "              dt.fit(x_train, y_train)\n",
        "              y_pred = dt.predict(x_test)\n",
        "              value = accuracy(y_pred, y_test)\n",
        "              if(value > max_accuracy):\n",
        "                  #print(\"RandomForest(criterion='gini', n_estimators=\"+str(rand_dt)+\", max_features=\"+str(feature_num)+\", boostrap=\"+str(boostrap)+\"), Accuracy Score:\", value)\n",
        "                  max_accuracy = value\n",
        "                  model = dt\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oQb3Vz4w3O8",
        "outputId": "4655d97f-b314-470a-e727-c8aac98f5cd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DecisionTree(criterion='gini', max_depth=1), Accuracy Score: 0.8933333333333333\n",
            "DecisionTree(criterion='gini', max_depth=3), Accuracy Score: 0.9166666666666666\n",
            "DecisionTree(criterion='gini', max_depth=5), Accuracy Score: 0.94\n",
            "RandomForest(criterion='gini', n_estimators=3, max_features=19, boostrap=True), Accuracy Score: 0.9566666666666667\n",
            "RandomForest(criterion='gini', n_estimators=9, max_features=14, boostrap=True), Accuracy Score: 0.96\n",
            "RandomForest(criterion='gini', n_estimators=9, max_features=20, boostrap=True), Accuracy Score: 0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "my_model = train_your_model(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egSKj1tqw3O8",
        "outputId": "fcd77703-acb3-4546-fc33-6fd06553d932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test-set accuarcy score:  0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "y_pred = my_model.predict(x_test)\n",
        "print('Test-set accuarcy score: ', accuracy(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "29Yi1Ca_w3O8",
        "outputId": "4562b69f-771d-488d-fcf5-6c757652d196"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-261606e5d981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "#len(y_pred)=300\n",
        "#assert y_pred.shape == (500, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkdhMyvpw3O8"
      },
      "source": [
        "## Supplementary\n",
        "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El3BTvCfw3O9"
      },
      "source": [
        "### DO NOT MODIFY CODE BELOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "2-RxHeCGw3O9",
        "outputId": "3dc7bc80-c5af-48c5-800e-de1d35aac7df"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'price_range'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-a9915a8dfd07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price_range'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test-set accuarcy score: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'price_range'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
        "\n",
        "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DM6Fb3mw3O9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9rSIG5xw3O9"
      },
      "outputs": [],
      "source": [
        "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict(x_test)\n",
        "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
        "        return score\n",
        "    else:\n",
        "        print(f\"{name} failed\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "def patient_checker(score, thres, CLS, kwargs, name,\n",
        "                    x_train, y_train, x_test, y_test, patient=10):\n",
        "    while patient > 0:\n",
        "        patient -= 1\n",
        "        clf = CLS(**kwargs)\n",
        "        clf.fit(x_train, y_train)\n",
        "        y_pred = clf.predict(x_test)\n",
        "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
        "            return score\n",
        "    print(f\"{name} failed\")\n",
        "    print(\"Considering the randomness, we will check it manually\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
        "    df = pd.read_csv(\n",
        "        file_url,\n",
        "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
        "    )\n",
        "\n",
        "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
        "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
        "\n",
        "    train_idx = range(0, len(df), 10)\n",
        "    test_idx = range(1, len(df), 20)\n",
        "\n",
        "    train_df = df.iloc[train_idx]\n",
        "    test_df = df.iloc[test_idx]\n",
        "\n",
        "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
        "    feature_names = x_train.columns.values\n",
        "    x_train = x_train.values\n",
        "    y_train = train_df['Target'].values\n",
        "\n",
        "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
        "    x_test = x_test.values\n",
        "    y_test = test_df['Target'].values\n",
        "    return x_train, y_train, x_test, y_test, feature_names\n",
        "\n",
        "\n",
        "score = 0\n",
        "\n",
        "data = np.array([1, 2])\n",
        "if abs(gini(data) - 0.5) < 1e-4:\n",
        "    score += 2.5\n",
        "else:\n",
        "    print(\"gini test failed\")\n",
        "\n",
        "if abs(entropy(data) - 1) < 1e-4:\n",
        "    score += 2.5\n",
        "else:\n",
        "    print(\"entropy test failed\")\n",
        "\n",
        "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
        "\n",
        "score += discrete_checker(5, 0.9337,\n",
        "                          DecisionTree(criterion='gini', max_depth=3),\n",
        "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "score += discrete_checker(2.5, 0.9036,\n",
        "                          DecisionTree(criterion='gini', max_depth=10),\n",
        "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "score += discrete_checker(2.5, 0.9096,\n",
        "                          DecisionTree(criterion='entropy', max_depth=3),\n",
        "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
        "\n",
        "score += patient_checker(\n",
        "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
        "    \"AdaBoost(n_estimators=10)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
        "    \"AdaBoost(n_estimators=100)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.91, RandomForest,\n",
        "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
        "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.91, RandomForest,\n",
        "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
        "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.92, RandomForest,\n",
        "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
        "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
        "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
        "print(\"*** This score is only for reference ***\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "57e3ffd0aa5f522d043ae6935574d5ca5b6b7a6179bb5e22d076dcaf64259c80"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
